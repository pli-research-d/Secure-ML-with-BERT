{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GJrY-ppZ8tZn"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = BertForSequenceClassification.from_pretrained('personalized-learning-model')\n",
        "    return model\n",
        "\n",
        "def average_models(model_paths):\n",
        "    # Load the models\n",
        "    models = [load_model(path) for path in model_paths]\n",
        "\n",
        "    # Initialize the averaged model with the same configuration as the first model\n",
        "    averaged_model = BertForSequenceClassification.from_pretrained('personalized-learning-model')\n",
        "\n",
        "    # Average the parameters\n",
        "    with torch.no_grad():\n",
        "        for name, param in averaged_model.named_parameters():\n",
        "            # Sum the parameters from each model\n",
        "            temp = sum(model.state_dict()[name] for model in models)\n",
        "            # Divide by the number of models to get the average\n",
        "            param.copy_(temp / len(models))\n",
        "\n",
        "    return averaged_model\n",
        "\n",
        "# Paths to your saved models\n",
        "model_path1 = 'personalized-learning-model'\n",
        "model_path2 = 'averaged-global-model'\n",
        "\n",
        "# Average the models\n",
        "averaged_model = average_models([model_path1, model_path2])\n",
        "\n",
        "# Save the averaged model (if needed)\n",
        "averaged_model.save_pretrained('averaged-global-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbwHNAUd-ys5",
        "outputId": "451705c0-f8a7-4ae9-a6a3-f684ad7fe77c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning Measures Scores: {'Conscientiousness': 5.16, 'Motivation': 6.17, 'Understanding': 5.98, 'Engagement': 6.17}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Inference Function\n",
        "def calculate_averaged_learning_measures(logins, time_spent, page_visits, search_queries, activity_completion, quiz_score, reactions_pos, reactions_neg, feedback):\n",
        "    input_text = f\"{logins} {time_spent} {page_visits} {search_queries} {activity_completion} {quiz_score} {reactions_pos} {reactions_neg} {feedback}\"\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertForSequenceClassification.from_pretrained('averaged-global-model')\n",
        "    tokens = tokenizer(input_text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "        scores = torch.sigmoid(output.logits).squeeze(0).cpu().numpy()\n",
        "\n",
        "    return {\n",
        "        'Conscientiousness': round(scores[0] * 10, 2),\n",
        "        'Motivation': round(scores[1] * 10, 2),\n",
        "        'Understanding': round(scores[2] * 10, 2),\n",
        "        'Engagement': round(scores[3] * 10, 2)\n",
        "    }\n",
        "\n",
        "# Example call to the function to demonstrate its use\n",
        "averaged_learning_scores = calculate_learning_measures(4, 9, 11, 5, 80.0, 84.0, 3, 2, 6)\n",
        "print(\"Learning Measures Scores:\", averaged_learning_scores)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
