{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gm7t-yLLrGU",
        "outputId": "9069116d-c6cf-495d-a4fe-e1227bc3cba6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Train Loss: -13.4922, Val Loss: -20.3699\n",
            "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [05:39<00:00, 339.59s/trial, best loss: -4435.16845703125]\n",
            "Best hyperparameters: {'batch_size': 0, 'epochs': 2, 'learning_rate': 0.01851008248976466}\n",
            "Best validation loss: -4435.16845703125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Train Loss: -4.9585, Val Loss: -2.7928\n",
            "Epoch 2/2, Train Loss: -4.6514, Val Loss: -2.7928\n",
            "Learning Measures Scores: {'Conscientiousness': 5.16, 'Motivation': 6.17, 'Understanding': 5.98, 'Engagement': 6.17}\n"
          ]
        }
      ],
      "source": [
        "# Secure ML Environment\n",
        "class SecureMLEnvironment:\n",
        "    \"\"\"Environment to securely handle data within isolated sandboxes.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.sandbox = {}\n",
        "\n",
        "    def isolate(self, data):\n",
        "        \"\"\"Isolate data to sandbox for secure handling.\"\"\"\n",
        "        data_id = f\"data_{len(self.sandbox)}\"\n",
        "        self.sandbox[data_id] = data\n",
        "        return data_id, self.sandbox[data_id]\n",
        "\n",
        "secure_env = SecureMLEnvironment()\n",
        "\n",
        "# Privacy Safeguard Functions\n",
        "class PrivacySafeguard:\n",
        "    \"\"\"Static methods for data anonymization and encryption.\"\"\"\n",
        "    key = Fernet.generate_key()\n",
        "    cipher = Fernet(key)  # Create cipher outside the class for security\n",
        "\n",
        "    @staticmethod\n",
        "    def anonymize(data):\n",
        "        return np.array(['ANONYMIZED' for _ in data])\n",
        "\n",
        "    @staticmethod\n",
        "    def encrypt_data(data):\n",
        "        return [PrivacySafeguard.cipher.encrypt(str(item).encode()).decode() for item in data]\n",
        "\n",
        "# Learning Dataset Class\n",
        "class LearningDataset(Dataset):\n",
        "    \"\"\"Custom dataset class for secure and private data handling in machine learning.\"\"\"\n",
        "    def __init__(self, data, targets, tokenizer, max_length=512):\n",
        "        data = PrivacySafeguard.anonymize(PrivacySafeguard.encrypt_data(data))\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length  # Consistent max_length across instances\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        label = self.targets[idx]\n",
        "        tokens = self.tokenizer(\n",
        "            ' '.join(map(str, item)),\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': tokens['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Model Setup\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "# Loading and preparing data\n",
        "\n",
        "csv_file_path = './sample_data/learner_behavior_data7.csv'\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load data from a CSV file and isolate it in a secure environment.\"\"\"\n",
        "    df = pd.read_csv(file_path, chunksize=1000)  # Efficient data loading\n",
        "    data = []\n",
        "    targets = []\n",
        "    for chunk in df:\n",
        "        data.extend(chunk.iloc[:, :-4].values)\n",
        "        targets.extend(chunk.iloc[:, -4:].values)\n",
        "    data_id, isolated_data = secure_env.isolate(data)\n",
        "    return isolated_data, targets\n",
        "\n",
        "training_data, target_scores = load_data(csv_file_path)\n",
        "\n",
        "# Data preparation\n",
        "dataset = LearningDataset(training_data, target_scores, tokenizer)\n",
        "train_size = int(0.75 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "# Optimizer and device setup\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training and validation loop\n",
        "def train_and_validate(model, train_loader, val_loader, device, num_epochs=1):\n",
        "    \"\"\"Train and validate the model.\"\"\"\n",
        "    personalized_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save the personalized model\n",
        "        if avg_val_loss < personalized_val_loss:\n",
        "            personalized_val_loss = avg_val_loss\n",
        "            model.save_pretrained('personalized-learning-model')\n",
        "            tokenizer.save_pretrained('personalized-learning-model')\n",
        "\n",
        "train_and_validate(model, train_loader, val_loader, device)\n",
        "\n",
        "# Hyperparameter Tuning Function\n",
        "def hyperparameter_tuning(data, targets, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Performs hyperparameter tuning for the machine learning model.\n",
        "\n",
        "    Args:\n",
        "        data: The training data.\n",
        "        targets: The target labels.\n",
        "        tokenizer: The tokenizer for text processing.\n",
        "        max_length: The maximum sequence length for input data.\n",
        "\n",
        "    Returns:\n",
        "        The best hyperparameter configuration and the corresponding model.\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(params):\n",
        "        \"\"\"\n",
        "        Objective function for hyperparameter optimization.\n",
        "\n",
        "        Args:\n",
        "            params: A dictionary of hyperparameter values.\n",
        "\n",
        "        Returns:\n",
        "            The validation loss of the model trained with the given hyperparameters.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create dataset and dataloaders\n",
        "        dataset = LearningDataset(data, targets, tokenizer, max_length)\n",
        "        train_size = int(0.75 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "        # Create model and optimizer\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "        # Train and validate the model\n",
        "        best_val_loss = float('inf')\n",
        "        for epoch in range(params['epochs']):\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "\n",
        "            for batch in train_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "\n",
        "        return {'loss': best_val_loss, 'status': STATUS_OK}\n",
        "\n",
        "    # Define hyperparameter search space\n",
        "    search_space = {\n",
        "        'learning_rate': hp.loguniform('learning_rate', -5, -3),\n",
        "        'batch_size': hp.choice('batch_size', [2, 4, 8]),\n",
        "        'epochs': hp.choice('epochs', [3, 5, 10])\n",
        "    }\n",
        "\n",
        "    # Run hyperparameter tuning\n",
        "    trials = Trials()\n",
        "    best_params = fmin(objective, search_space, algo=tpe.suggest, max_evals=1, trials=trials)\n",
        "\n",
        "    # Print best hyperparameters and validation loss\n",
        "    print('Best hyperparameters:', best_params)\n",
        "    print('Best validation loss:', trials.best_trial['result']['loss'])\n",
        "\n",
        "    # Create and train the model with the best hyperparameters\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'])\n",
        "    train_and_validate(model, train_loader, val_loader, device, num_epochs=best_params['epochs'])\n",
        "\n",
        "    return model, best_params  # Return both model and hyperparameters\n",
        "\n",
        "# Call the hyperparameter tuning function\n",
        "model, best_params = hyperparameter_tuning(training_data, target_scores, tokenizer)\n",
        "\n",
        "# Note: Additional functions for averaging models or further uses could be added similarly.\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Inference Function\n",
        "def calculate_learning_measures(logins, time_spent, page_visits, search_queries, activity_completion, quiz_score, reactions_pos, reactions_neg, feedback):\n",
        "    input_text = f\"{logins} {time_spent} {page_visits} {search_queries} {activity_completion} {quiz_score} {reactions_pos} {reactions_neg} {feedback}\"\n",
        "    tokens = tokenizer(input_text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "        scores = torch.sigmoid(output.logits).squeeze(0).cpu().numpy()\n",
        "\n",
        "    return {\n",
        "        'Conscientiousness': round(scores[0] * 10, 2),\n",
        "        'Motivation': round(scores[1] * 10, 2),\n",
        "        'Understanding': round(scores[2] * 10, 2),\n",
        "        'Engagement': round(scores[3] * 10, 2)\n",
        "    }\n",
        "\n",
        "# Example call to the function to demonstrate its use\n",
        "learning_scores = calculate_learning_measures(4, 9, 11, 5, 80.0, 84.0, 3, 2, 6)\n",
        "print(\"Learning Measures Scores:\", learning_scores)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
